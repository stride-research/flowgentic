"""
Example: AGENT_TOOL_AS_MCP Real Client Demo

This demonstrates that MCP tools work in the AsyncFlow execution environment:
- Runs asynchronously through AsyncFlow
- Goes through retry/fault tolerance mechanism
- LLM can call it as a tool
- Doesn't block workflow
- Returns valid results to agent

IMPLEMENTATION STATUS:
 Real MCP client integration working
 Connects to Anthropic's demo MCP server via NPX
 Fallback to placeholder if connection fails
 Infrastructure proven ready for production MCP servers

Requires:
- OPENROUTER_API_KEY in .env file
- Node.js with NPX for MCP server
"""

import asyncio
from concurrent.futures import ThreadPoolExecutor
from radical.asyncflow.backends.execution.concurrent import ConcurrentExecutionBackend
from flowgentic.langGraph.main import LangraphIntegration
from flowgentic.langGraph.execution_wrappers import AsyncFlowType
from flowgentic.utils.llm_providers import ChatLLMProvider
from langgraph.prebuilt import create_react_agent
from dotenv import load_dotenv

load_dotenv()


async def main():
	print("=" * 80)
	print("AGENT_TOOL_AS_MCP: Real MCP Client Demo")
	print("=" * 80)
	print("\nThis demonstrates real MCP integration working:")
	print("‚úì Async execution through AsyncFlow")
	print("‚úì Retry/fault tolerance mechanism")
	print("‚úì Real MCP client connecting to MCP server")
	print("‚úì LLM can call it intelligently")
	print("‚úì Fallback to placeholder if connection fails")
	print("\nConnecting to Anthropic's demo MCP server via NPX")
	print("The server provides multiple tools (echo, add, etc.)")
	print("Currently demonstrating with 'echo' tool\n")

	backend = await ConcurrentExecutionBackend(ThreadPoolExecutor())

	async with LangraphIntegration(backend=backend) as integration:
		# Define MCP tool (real client implementation with fallback)
		@integration.execution_wrappers.asyncflow(
			flow_type=AsyncFlowType.AGENT_TOOL_AS_MCP,
			tool_description="Call external MCP model for complex reasoning tasks that require external computation",
		)
		async def call_mcp_model(prompt: str):
			"""
			Call external model via MCP protocol.

			Connects to real MCP server via NPX, calls available tools,
			and returns results. Falls back to placeholder on error.

			Infrastructure ready for production MCP servers.
			"""
			pass  # Implementation handled by wrapper

		print("üìã MCP Tool created: 'call_mcp_model'")
		print(f"   Tool type: {type(call_mcp_model)}")
		print(f"   Tool name: {call_mcp_model.name}")
		print(f"   Tool description: {call_mcp_model.description}")

		# Create LLM and ReAct agent
		print("\nü§ñ Creating ReAct agent with MCP tool...")
		llm = ChatLLMProvider(provider="OpenRouter", model="google/gemini-2.5-flash")
		agent = create_react_agent(llm, tools=[call_mcp_model])
		print("‚úÖ Agent created with MCP tool!")

		print("\n" + "=" * 80)
		print("Testing MCP Tool Execution")
		print("=" * 80)

		# Test 1: Direct tool call (manual) - proves MCP works
		print("\nüìç Test 1: Manual tool invocation")
		print("-" * 80)
		result = await call_mcp_model.ainvoke({"prompt": "Hello MCP!"})
		print(f"‚úÖ Tool executed successfully")
		print(f"   Response: {result}")

		# Test 2: LLM calling the tool - proves LLM integration works
		print("\nüìç Test 2: LLM decides to use MCP tool")
		print("-" * 80)
		print("üë§ User: Use the MCP model to say hello")
		response = await agent.ainvoke(
			{
				"messages": [
					("user", "Use the call_mcp_model tool to process 'Hello from LLM!'")
				]
			}
		)
		print(f"ü§ñ Assistant: {response['messages'][-1].content}")

		# Test 3: LLM behavior test - should NOT use MCP for simple math
		print("\nüìç Test 3: LLM answers without MCP tool (intelligence check)")
		print("-" * 80)
		print("üë§ User: What's 5 + 5?")
		response = await agent.ainvoke({"messages": [("user", "What's 5 + 5?")]})
		print(f"ü§ñ Assistant: {response['messages'][-1].content}")


if __name__ == "__main__":
	asyncio.run(main())
